{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 17: Introduction to Statistical Inference\n",
    "\n",
    "Plan for today:\n",
    "- Review for loops and writing functions\n",
    "- Introduction to statistical inference\n",
    "- Introduction to hypothesis tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(17)   # get class code    \n",
    "# YData.download.download_class_code(17, TRUE) # get the code with the answers \n",
    "\n",
    "# YData.download.download_class_file('project_template.ipynb', 'homework')  # downloads the class project template \n",
    "# YData.download_homework(7)  # downloads the 7th homework \n",
    "\n",
    "YData.download_data(\"movies.csv\")\n",
    "YData.download_data(\"daily_bike_totals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, please run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/lederman/YData_package/tarball/master\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Warm-up exercises using for loops\n",
    "\n",
    "As we discussed last class, loops allow us to repeat a process many times. They are particularly useful in conjuction with lists to process and store multiple values. \n",
    "\n",
    "Let's start with a quick warm up exercise on for loops. The code below loads our bike data from 2014 and can creates two lists which are:\n",
    "\n",
    "- `weekday`: A list of Booleans specifying whether a day is a weekday\n",
    "- `num_trips`: A list containing how many trips were taken on each day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bikes = pd.read_csv(\"daily_bike_totals.csv\", parse_dates = [0])\n",
    "bikes_2014 = bikes.query(\"date > '2013-12-31'\").query(\"date < '2015-01-01'\")\n",
    "\n",
    "weekday = bikes_2014[\"weekday\"].to_list()\n",
    "num_trips = bikes_2014[\"trips\"].to_list()\n",
    "\n",
    "print(weekday[0:5])\n",
    "print(num_trips[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up exercise 0.1.1\n",
    "\n",
    "Please use a for loop to a list called `weekday_trips` which contains the number of trips that occurred only on weekdays. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative solution using the zip() function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up exercise 0.1.2\n",
    "\n",
    "Now add to your code so that you also create a list called `weekend_trips` that contain the number of trips that occured on all weekend. \n",
    "\n",
    "Once you have created these lists create side-by-side boxplots to compare the number of trips taken on weekdays and weekends. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup 0.2 - Functions\n",
    "\n",
    "### 0.2.1: A function\n",
    "\n",
    "Write a function named `string_and_length` which takes a string and returns a string containing the original string and its length.\n",
    "For example, `string_and_length(\"ABC\")` would return the string `\"ABC3\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"This is my string\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.2: Loop over the function\n",
    "\n",
    "Write code that takes the string `s` below, strips the spaces, and produces a string that contains each word of the original string, followed by the length of that word.\n",
    "\n",
    "For example, if `s = \"This is my string\"` then, `newstring = \"This4is2my2string6\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistical inference\n",
    "\n",
    "In statistical inference we use a smaller sample of data to make claims about a larger population of data. \n",
    "\n",
    "As an example, let's look at the [2020 election](https://www.cookpolitical.com/2020-national-popular-vote-tracker) between Donald Trump and Joe Biden, and let's focus on the results from the state of Georgia. After all the votes had been counted, the resuts showed that:\n",
    "\n",
    "- Biden received 2,461,854 votes\n",
    "- Trump received 2,473,633 votes\n",
    "\n",
    "Since we have all the votes on election data, we can precisely calculate the population parameter of the proportion of votes that Biden received, which we will denote with the symbol $\\pi_{Biden}$. \n",
    "\n",
    "Let's create names `num_trump_votes` and `num_biden_votes`, and calculate `true_prop_Biden` which is the value $\\pi_{Biden}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trump_votes = 2461854  # 2,461,854\n",
    "num_biden_votes = 2473633  # 2,473,633\n",
    "\n",
    "\n",
    "# calculate the proportion of people who voted for Biden\n",
    "true_prop_Biden = num_biden_votes/(num_biden_votes + num_trump_votes)\n",
    "\n",
    "true_prop_Biden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a DataFrame called `georgia_df` that captures these election results. Each row in the DataFrame represents a votes. The column `Voted Biden` is `True` if a voter voted for Biden and `False` if the voter voted for Trump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_votes = np.repeat(True, num_biden_votes)     # create 2,473,633 Trues for the Biden votes\n",
    "trump_votes = np.repeat(False, num_trump_votes)    # create 2,461,854 Falses for the Trump votes\n",
    "election_outcome = np.concatenate((biden_votes, trump_votes))  # put the votes together\n",
    "\n",
    "georgia_df = pd.DataFrame({\"Voted Biden\": election_outcome})  # create a DataFrame with the data\n",
    "georgia_df = georgia_df.sample(frac = 1)   # shuffle the order to make it more realistic\n",
    "\n",
    "georgia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we didn't know the actual value of $\\pi_{Biden}$ and we wanted to estimate it based on a poll of 1,000 voters. We can simulate this by using the pandas `.sample(n = )` method.\n",
    "\n",
    "Let's simulate sampling random voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10 random points\n",
    "georgia_df.sample(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate proportions of voters that voted for Biden - i.e., p-hats\n",
    "\n",
    "one_sample = georgia_df.sample(1000)\n",
    "\n",
    "np.mean(one_sample['Voted Biden'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Creating a sampling distribution via taking random samples\n",
    "\n",
    "Suppose 100 polls were conducted. How many of them would show that Biden would get the majority of the vote? \n",
    "\n",
    "Let's simulate this \"sampling distribution\" of statistics now... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_size = 1000\n",
    "num_simulations = 100\n",
    "\n",
    "sampling_dist = []\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the sampling distribution\n",
    "\n",
    "plt.hist(sampling_dist, edgecolor = \"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Creating a sampling distributions via coin flips (a faster way to simulate data)\n",
    "\n",
    "Rather than simulating polling outcomes by pulling random samples from a DataFrame, let's simulate each vote by simulating randomly flipping a coin, where the probability of getting a \"Head\" (True value) is the probability of Biden getting a vote.\n",
    "\n",
    "To do this we can use our `flip_coins(n, prob_heads, return_prop)` function we wrote in the previous lecture. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for teh previous lecture:\n",
    " \n",
    "def flip_coins(n, prob, return_prop = False):\n",
    "    \n",
    "    rand_nums = np.random.rand(n)\n",
    "    num_heads = np.sum(rand_nums <= prob)\n",
    "    \n",
    "    if return_prop:\n",
    "        return num_heads/n\n",
    "    else:\n",
    "        return num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# sampling distribution of many polls conducted\n",
    "\n",
    "sample_size = 1000\n",
    "num_simulations = 100\n",
    "\n",
    "sampling_dist = []\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sampling_dist, edgecolor = \"black\", bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis tests\n",
    "\n",
    "In hypothesis testing, we start with a claim about a population parameter (e.g., µ = 4.2, or π = 0.25).\n",
    "\n",
    "This claim implies we should get a certain distribution of statistics, called \"The null distribution\". \n",
    "\n",
    "If our observed statistic is highly unlikely to come from the null distribution, we reject the claim. \n",
    "\n",
    "We can break down the process of running a hypothesis test into 5 steps. \n",
    "\n",
    "1. State the null and alternative hypothesis\n",
    "2. Calculate the observed statistic of interest\n",
    "3. Create the null distribution \n",
    "4. Calculate the p-value \n",
    "5. Make a decision\n",
    "\n",
    "Let's run through these steps now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: State the null and alternative hypothesis\n",
    "\n",
    "$H_0: \\pi = 0.5$\n",
    "\n",
    "$H_A: \\pi < 0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate the observed statistic of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce data to a smaller number of columns: \"title\" and \"binary\"\n",
    "\n",
    "movies_smaller = movies[[\"title\", \"binary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of movies that pass the Bechdel test\n",
    "\n",
    "booleans_passed = ...\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create the null distribution \n",
    "\n",
    "We need to create a null distribution, which is the distribution of statistics we would expect to get if the null hypothesis is true. \n",
    "\n",
    "**Question**: about what percent of the movies would we expect to pass the Bechdel test if the null distribution was true? \n",
    "\n",
    "**Answer**: 50%\n",
    "\n",
    "Let's create simulated data that is consistent with this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate one proportion consistent with the null hypothesis\n",
    "\n",
    "# get the total number of movies in our dataset n\n",
    "n = movies.shape[0]\n",
    "print(n)\n",
    "\n",
    "# proportion consistent with the null hypothesis\n",
    "null_prop = .5\n",
    "\n",
    "\n",
    "# one statistic consistent with null hypothesis\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a null distribution \n",
    "\n",
    "null_dist = []\n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the null distribution \n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\", bins = 20) #, range = (.4, .6));\n",
    "plt.plot(prop_passed, 30, '.', markersize = 30, color = \"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Calculate the p-value \n",
    "\n",
    "Calculate the proportion of points in the null distribution that are more extreme than the observed statistic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-value\n",
    "\n",
    "# create a Boolean vector indicating whether each p-hat in the null distribution \n",
    "# was greater than the observed proportion that passed the Bechdel test\n",
    "stats_more_extreme = ...\n",
    "\n",
    "print(stats_more_extreme[0:5])\n",
    "\n",
    "# calculate the p-value \n",
    "p_value = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Make a decision\n",
    "\n",
    "Since the p-value is very small (essentially zero) it is very unlikely that our statistic come from the null distribution. Thus we will reject the null hypothesis and conclude that less than 50% of movies pass the Bechdel test. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ydata_2025spring_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
