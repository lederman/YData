{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 20: Hypothesis tests and Confidence intervals\n",
    "\n",
    "Plan for today:\n",
    "- Hypothesis tests for correlation\n",
    "- Visual hypothesis tests\n",
    "- Two-sided hypothesis tests\n",
    "- Confidence intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(20)   # get class code    \n",
    "# YData.download.download_class_code(20, TRUE) # get the code with the answers \n",
    "\n",
    "YData.download_data(\"babies.csv\")\n",
    "\n",
    "YData.download_data(\"amazon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, you should run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/lederman/YData_package/tarball/master\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis test for two means: Smoking and baby weights\n",
    "\n",
    "The Child Health and Development Studies investigate a range of topics. One study, in particular, considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area.\n",
    "\n",
    "Let's examine this data to see if the average weight of babies of babies is different depending on whether the mother of the baby smokes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: State the null and alternative hypotheses\n",
    "\n",
    "\n",
    "$H_0$: ...\n",
    "\n",
    "$H_A$: ...\n",
    "\n",
    "\n",
    "Where $\\mu_{non-smoke}$ and $\\mu_{smoke}$ are the population means of babies born to mothers who did not smoke and who smoked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the observed statistic\n",
    "\n",
    "The code below loads the data from the study. The two relevant columns are:\n",
    "- `bwt`: The birth weight of the baby in ounces\n",
    "- `smokes`: whether the mother smokes (1) or does not smoke (0)\n",
    "\n",
    "More information about the data is available at: https://www.openintro.org/data/index.php?data=babies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babies = pd.read_csv(\"babies.csv\")\n",
    "\n",
    "babies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the analysis, create a new DataFrame called `babies2` that only has the smoke and bwt columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame called babies2 that has only the smoke and bwt columns\n",
    "babies2 = ...\n",
    "babies2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have our observed statistic be the different of sample means  $\\bar{x}_{non-smoke} - \\bar{x}_{smoke}$.  \n",
    "\n",
    "Please calculate this observe statistic and save it to the name `obs_stat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = ...\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_stat_series = ...\n",
    "\n",
    "obs_stat = ...\n",
    "\n",
    "obs_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the rest of the analysis easier, write a function `get_diff_baby_weights(babies_df)` that will take a DataFrame `babies_df` that has smoke and btw information will return the difference in the means of babies that have mothers who to not smoke and those who do smoke. \n",
    "\n",
    "Also, test the function to make sure it give the same observed statistic you calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_baby_weights(babies_df):\n",
    "    ...\n",
    "\n",
    "# get that the function works\n",
    "\n",
    "get_diff_baby_weights(babies2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the null distribution \n",
    "\n",
    "Now let's create a null distribution that has 10,000 statistics that are consistent with the null hypothesis. \n",
    "\n",
    "In this example, if the null hypothesis was true, then there would be no difference between the smoking mothers and the non-smoking mothers. Thus, under the null hypothesis, we can shuffle the group labels and get equally valid statistics. \n",
    "\n",
    "Let's create one statistic consistent with the null distribution to understand the process. We can then repeat this 10,000 times to get a full null distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data \n",
    "\n",
    "shuff_babies = ...\n",
    "\n",
    "get_diff_baby_weights(shuff_babies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create a full null distribution \n",
    "\n",
    "null_dist = []\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the null distribution \n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\", bins = 100);\n",
    "\n",
    "\n",
    "# put a line at the observed statistic value\n",
    "\n",
    "plt.axvline(obs_stat, color = \"red\");\n",
    "plt.xlabel(\"prop treat - prop control\");\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate the p-value\n",
    "\n",
    "The p-value is the proportion of points in the null distribution that are more extreme than the observed statistic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = ...\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Draw a conclusion\n",
    "\n",
    "Since the p-value is less than the typical significance level of 0.05, we can reject the null hypothesis. This we conclude there is a difference between in the average weight of babies born to mothers who smoke compared to mothers who do not smoke. \n",
    "\n",
    "However, because this was an observational study, where mothers were **not** randomly assigned to treatment and control groups, we can conclude that smoking does causes babies to have less weight when they are born. In particular, it is possible there could be other \"lurking/confounding\" variables that cause both the mothers to smoke, and the babies to have less weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center>\n",
    "<img src=\"https://i.ytimg.com/vi/x4c_wI6kQyE/maxresdefault.jpg\" alt=\"smoking\" style=\"width: 300px;\"/>\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis tests for correlation\n",
    "\n",
    "Let's run a hypothesis tests for correlation to see if books that have more pages cost more! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: State the null and alternative hypotheses\n",
    "\n",
    "**In words** \n",
    "\n",
    "Null hypthesis: There is no correlation between the number of pages in a book and the cost of the book. \n",
    "\n",
    "Alternative hypothesis: Books that have more pages cost more. \n",
    "\n",
    "**In symbols**\n",
    "\n",
    "$H_0$: ...\n",
    "\n",
    "$H_A$: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the observed statistic\n",
    "\n",
    "Data on 230 books from Amazon.com are loaded below. Let's calculate the observed correlation ($r$) between the number of pages in the book (`NumPages`) and the listed price (`List.Price`). \n",
    "\n",
    "To make your life easier, first just reduce the data set to only the `NumPages` and `List.Price` columns and save this to a DataFrame called `amazon_smaller`. Then use `amazon_smaller` for the rest of this problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.read_csv(\"amazon.csv\")\n",
    "\n",
    "amazon.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "amazon_smaller = amazon[[\"NumPages\", \"List.Price\"]]\n",
    "\n",
    "obs_stat = ...\n",
    "\n",
    "obs_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the null distribution \n",
    "\n",
    "How can we create one statistic consistent with the null hypothesis? \n",
    "\n",
    "See if you can create one statistic consistent with the null distribution below. Once you have done that, create the full null distribution!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one statistic consistent with the null distribution\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the full null distribution\n",
    "\n",
    "null_dist = []\n",
    "\n",
    "for i in range(10000):  \n",
    "        \n",
    "    ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the null distribution \n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\", bins = 100);\n",
    "\n",
    "\n",
    "# put a line at the observed statistic value\n",
    "\n",
    "plt.axvline(obs_stat, color = \"red\");\n",
    "plt.xlabel(\"r\");\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate the p-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = ...\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Draw a conclusion\n",
    "\n",
    "The p-value is small (less than the convensional level of 0.05) so we would reject the null hypothesis and conclude that there is a correlation between the number of pages in a book and the price of a book at the population level. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing hypothesis tests for correlation\n",
    "\n",
    "We can also run a visual hypothesis test for correlation by creating a visual lineup that displays several scatter plots of shuffled data and one scatter plot of the real data. If you can tell which plot contains the real (unshuffled) data, this corresponds to being able to reject the null hypothesis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(123)\n",
    "\n",
    "def create_lineup(the_data): \n",
    "    \n",
    "    real_plot_num = np.random.permutation(range(20))[0]\n",
    "    lineup_data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(20):\n",
    "        \n",
    "        if i == real_plot_num:\n",
    "            curr_data = the_data.copy()\n",
    "        \n",
    "        else:\n",
    "            curr_data = pd.DataFrame(the_data.iloc[:, 1], the_data.iloc[:, 0]).reset_index()\n",
    "            curr_data[curr_data.columns[1]] = np.random.permutation(curr_data[curr_data.columns[1]])\n",
    "        \n",
    "        curr_data.loc[:, \"Lineup\"] = i\n",
    "        lineup_data = pd.concat([lineup_data, curr_data])\n",
    "        \n",
    "    lineup_data = lineup_data.reset_index()\n",
    "\n",
    "    sns.set_context(\"paper\", rc={\"axes.labelsize\":20})   \n",
    "    sns.relplot(lineup_data, x = lineup_data.columns[1], y = lineup_data.columns[2], col = \"Lineup\", col_wrap=5); \n",
    "\n",
    "    return real_plot_num\n",
    "\n",
    "\n",
    "the_answer = create_lineup(amazon_smaller)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the answer\n",
    "\n",
    "# the_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Two-sided hypothesis test\n",
    "\n",
    "Sometime in hypothesis testing we don't know the direction of an effect, we only know that the null hypothesis is incorrect. \n",
    "\n",
    "In these circumstances, we write our alternative hypothesis such that we state that the parameter value is not equal to the value specified by the null hypothesis.\n",
    "\n",
    "For the testing whether there is a correlation between the number of pages in a book and price we can write our hypotheses as:\n",
    "\n",
    "$H_0: \\rho = 0$   \n",
    "\n",
    "i.e., the null hypothesis is the same as before.\n",
    "\n",
    "$H_A: \\rho \\ne 0$\n",
    "\n",
    "We now use not equal to ($\\ne$) in our alternative hypothesis.  \n",
    "\n",
    "To calculate the p-value, we need to look at the values more extreme than the observed statistic in in both tails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the null distribution \n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\", bins = 100);\n",
    "\n",
    "\n",
    "# put lines showing values more extreme than the observed statistic\n",
    "\n",
    "plt.axvline(obs_stat, color = \"red\");\n",
    "plt.axvline(-1 * obs_stat, color = \"red\");\n",
    "\n",
    "plt.xlabel(\"Null statistic value\");\n",
    "plt.ylabel(\"Count\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the p-value, we need to get the proportion of statistics in the null distribution that are more extreme than the observed statistic from both tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_right = np.mean(np.array(null_dist) >= obs_stat)\n",
    "p_value_left = np.mean(np.array(null_dist) <= -1 * obs_stat)\n",
    "\n",
    "p_value = p_value_right + p_value_left\n",
    "\n",
    "p_value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using hypothesis tests to generate confidence intervals\n",
    "\n",
    "There are several methods we that can be used to calculate confidence intervals, including using a computational method called the \"bootstrap\" and using \"parametric methods\" that involve using probability distributions. If you take a traditional introductory statistics class you will learn some of these methods.\n",
    "\n",
    "Below we use a less conventional method to calculate confidence intervals by looking at all parameters values that a hypothesis test fails to reject (at the p-value < 0.05 level). As you will see, the method gives similar results to other methods, although it requires a bit more computation time.\n",
    "\n",
    "As an example, let's create a confidence interval for the population proportion of movies $\\pi$ that pass the Bechdel test. As is the case for all confidence intervals, this confidence interval gives a range of plausible values that likely contains the true population proportion $\\pi$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, let's use a function that generates a statistic p-hat that is consistent with a particular population parameter value pi\n",
    "\n",
    "def generate_proportion(n, prob_heads):\n",
    "    \n",
    "    random_sample = np.random.rand(n) <= prob_heads\n",
    "    return np.mean(random_sample)\n",
    "\n",
    "generate_proportion(1794, .5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below calculates a p-value for the Bechdel data based on a particular pi value that is specified in a null hypothesis.\n",
    "# (i.e., it is a function that encapsulates the hypothesis test you ran in class 20).\n",
    "\n",
    "\n",
    "def get_Bechdel_pvalue(null_hypothesis_pi, plot_null_dist = False):\n",
    "    \n",
    "    \n",
    "    # The observed p-hat value\n",
    "    prop_passed = 803/1794\n",
    "    \n",
    "    \n",
    "    # Generate the null distribution \n",
    "    null_dist = []\n",
    "    \n",
    "    for i in range(10000):    \n",
    "        null_dist.append(generate_proportion(1794, null_hypothesis_pi))\n",
    "    \n",
    "    \n",
    "    # Calculate a \"two-tailed\" p-value which is the proportion of statistcs more extreme than the observed statistic\n",
    "\n",
    "    statistic_deviation = np.abs(null_hypothesis_pi - prop_passed)  # distance between p-hat and pi_0\n",
    "    \n",
    "    pval_left = np.mean(np.array(null_dist) <= null_hypothesis_pi - statistic_deviation)\n",
    "    pval_right = np.mean(np.array(null_dist) >= null_hypothesis_pi + statistic_deviation)    \n",
    "    \n",
    "    p_value = pval_left + pval_right\n",
    "\n",
    "\n",
    "    \n",
    "    # plot the null distribution and lines indicating values more extreme than the observed statistic \n",
    "    if plot_null_dist:\n",
    "        \n",
    "        plt.hist(null_dist, edgecolor = \"black\", bins = 30);\n",
    "        plt.axvline(null_hypothesis_pi - statistic_deviation, color = \"red\");\n",
    "        plt.axvline(null_hypothesis_pi + statistic_deviation, color = \"red\");\n",
    "        plt.axvline(null_hypothesis_pi, color = \"yellow\");\n",
    "\n",
    "        \n",
    "        plt.title(\"Pi-null is: \" + str(null_hypothesis_pi) + \"      \"  +\n",
    "                  \"p-value is: \" + str(round(p_value, 5)))\n",
    "      \n",
    "    # return the p-value\n",
    "    return p_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function with the value H0: pi = .5  (as we did in class 20)\n",
    "\n",
    "\n",
    "# test the function with the value H0: pi = .45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a range range of H0: pi = x  values\n",
    "\n",
    "possible_null_pis = ...\n",
    "\n",
    "possible_null_pis    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# get the p-value for a range of H0: pi = x  values\n",
    "\n",
    "\n",
    "pvalues = []\n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the p-values \n",
    "# convention calls a p-value < 0.05 is \"statistically significant\" indicating a pi imcompatible with the null hypothesis\n",
    "# our confidence interval is all pi values that are not statistically significant (i.e., pi values that are consistent with particular H0)\n",
    "\n",
    "pvalue_df = ...\n",
    "\n",
    "pvalue_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot p-values as as function of H0 pi's\n",
    "\n",
    "sns.relplot(pvalue_df, x = 'pi', y = 'p-values');\n",
    "plt.xticks(rotation=90);\n",
    "plt.axhline(.05, color = \"red\");\n",
    "\n",
    "plt.plot(pvalue_df['pi'], pvalue_df['non-significant'], color = \"green\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all plausible Pi values\n",
    "fail_to_reject_pis = ...\n",
    "\n",
    "fail_to_reject_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the CI as the max and min plausible pi values \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the statsmodels package to compute a confidence interval for a proportion\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "ci_low, ci_upp = sm.stats.proportion_confint(803, 1794, alpha=0.05, method='normal')\n",
    "(round(ci_low, 3), round(ci_upp, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ydata_2025spring_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
