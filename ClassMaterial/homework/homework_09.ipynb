{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9: Classification\n",
    "\n",
    "Welcome to the ninth and final homework! \n",
    "\n",
    "In this homework you practice using pattern classifiers to make predictions.\n",
    "\n",
    "Please complete this notebook by filling in the cells provided. \n",
    "\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "This assignment is due **Sunday April 20th at 11pm.** You can turn in the assignment up to 24 hours late for 90% credit (after that, the homework will only be accepted with a dean's excuse). \n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Refer to the policies page to learn more about how to learn cooperatively.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on. You can also post questions or start discussions on Ed Discussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "In order to complete the homework, it is necessary to download a few files. Please run the code below **only once** to download data needed to complete the homework. To run the code, click in the cell below and press the play button (or press shift-enter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running this notebook in colabs, please uncomment and run the following two lines\n",
    "# !pip install https://github.com/lederman/YData_package/tarball/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run this code once to download the files you will need to complete the homework \n",
    "\n",
    "import YData \n",
    "\n",
    "YData.download_data(\"college_scorecard_subset_2021_2022.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quote and reaction\n",
    "\n",
    "For the final quote and reaction please read this New York Times article on \"How Companies Learn your Secrets\". This was an early article which described how companies were using machine learning to better target their customers. Perhaps now that social media is so pervasive everyone is aware of how companies track us, but at the time of this publication this was relatively new. I will be interested in hearing your thoughts on this article! \n",
    "\n",
    "Note: Because the article is a bit long, I have grayed out certain regions of the article which you can skip (i.e., you can only read the parts that have a white background). However, if you are interested in the cognitive neuroscience of habit formation, you might find the other sections interesting as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 0.1 (5 points)**  Please write down your \"quote and reaction\" here.\n",
    "\n",
    "*Quote:*  ...\n",
    "\n",
    "Reaction: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports functions from packages we will use below.\n",
    "# Please run it each time you load the Jupyter notebook\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using a KNN classifier to predict whether a college is a liberal arts college or an R1 university \n",
    "\n",
    "On homework 7, you used the [College Scorecard data](https://collegescorecard.ed.gov/data/) to test a hypothesis.\n",
    "\n",
    "For this assignment, we will again focus just on R1 research universities (`CCBASIC` value of 15) and liberal arts colleges (`CCBASIC` value of 21) which contain institutions which are some of the most selective colleges in the United States. We will also use just a subset of features which are described below. A DataFrame called `scorecard` that has just the relevant data is created for you in the cell below. \n",
    "\n",
    "\n",
    "#### Features\n",
    "\n",
    "- `CCBASIC`: Carnegie Classification. You will try to predict whether a college is a \"Doctoral Universities: Very High Research Activity\" (value of 15) or a \"Baccalaureate Colleges: Arts & Sciences Focus\" (value of 21).  \n",
    "\n",
    "- `ADM_RATE`: Admission rate\n",
    "\n",
    "- `SAT_AVG`: Average SAT equivalent score of students admitted\n",
    "\n",
    "- `TUITIONFEE_OUT`: Out-of-state tuition and fees cost\n",
    "\n",
    "- `TUITFTE`: Net tuition revenue per full-time equivalent student\n",
    "\n",
    "- `INEXPFTE`: Instructional expenditures per full-time equivalent student\n",
    "\n",
    "- `AVGFACSAL`: Average faculty salary\n",
    "\n",
    "- `PFTFAC`: Proportion of faculty that is full-time\n",
    "\n",
    "- `C100_4`: Completion rate for first-time, full-time students at four-year institutions (100% of the expected time to completion). 100 percent of normal time is typically 4 years.\n",
    "\n",
    "- `GRAD_DEBT_MDN`: The median debt for students who have completed\n",
    "\n",
    "- `PCIP27`: Percentage of degrees awarded in Mathematics And Statistics\n",
    "\n",
    "- `UGDS_WOMEN`: Total share of enrollment of undergraduate degree-seeking students who are women\n",
    "\n",
    "- `ROOMBOARD_ON`: Cost of attendance: on-campus room and board cost\n",
    "\n",
    "- `MD_EARN_WNE_MALE0_P10`: Median earnings of non-male students working and not enrolled 10 years after entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorecard = pd.read_csv(\"college_scorecard_subset_2021_2022.csv\")\n",
    "\n",
    "scorecard = scorecard.query(\"(CCBASIC ==  15) or (CCBASIC == 21)\")\n",
    "\n",
    "scorecard = scorecard[['CCBASIC', 'ADM_RATE', 'SAT_AVG', 'TUITIONFEE_OUT', 'TUITFTE',\n",
    "                         'INEXPFTE', 'AVGFACSAL', 'PFTFAC', 'C100_4', 'PCIP27', 'UGDS_WOMEN',\n",
    "                         'ROOMBOARD_ON', 'MD_EARN_WNE_MALE0_P10']].dropna()\n",
    "\n",
    "# predict liberal arts college (21) vs. R1 institution (15)\n",
    "scorecard.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (4 points)**:  Before we dive into using machine learning to make predictions, let's do another data visualization of the data to try to get a sense of which features might be useful for separating these types of colleges. \n",
    "\n",
    "Please create a \"pair plot\" of scatter plots between all pairs of columns in a data set using the seaborn function `sns.pairplot()`. Recall that the first argument to this function is a DataFrame that we would like to plot. Also set the `hue` argument to the `CCBASIC` variable so that the points are colored based on the type of college. Then in the answer section below, describe a couple of features that seem like they might be useful for distinguishing between liberal arts and R1 colleges. \n",
    "\n",
    "Hint: you might have to squint a bit to see the axis labels to axis labels. If they are too hard to see you could split the data into two DataFrames (if you do this you will lose the ability to see the relationship between all pairs of features but you should still be able to get a sense of which features are useful for discriminating between these types of schools). \n",
    "\n",
    "Note, this code takes a while to run since it is plotting a lot of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (4 points)**: In order to use scikit-learn's machine learning algorithms we need to split apart our data into the \"labels\" that we are trying to predict and the \"features\" that we will use to make our predictions. To do this, start with the `scorecard` DataFrame and split it into a Series called `labels_y` that has our classification labels, and into a DataFrame called `features_X` that has our features. \n",
    "\n",
    "Once you have done this, print the first 3 rows of the `features_X` DataFrame to show your code worked correctly. \n",
    "\n",
    "Hint: The pandas `df.drop()` method could be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 (4 points)**: Let's now count how many examples there are in each of our two classes (i.e., how many examples there are of liberal arts colleges, and how many examples there are of R1 universities). To do this, call the `.value_counts()` method on the `labels_y` Series, and save the results to the name `class_counts` and print the results. \n",
    "\n",
    "After you have done this, in the answer section below, please answer the following questions: \n",
    "\n",
    "1. Suppose you had to guess which class a randomly selected data points belonged to, but were not given any of the features values. What would be the best strategy to use to guess which class the point belonged to in order to get the most correct guesses?\n",
    "\n",
    "2. If you used the best strategy to guess the class of those points (with no knowledge of the features) what percentage of the points would you guess correctly?  Note: this is the \"baseline\" accuracy we should expect a randomly guessing classifier to get correct and we should expect a real classifier to do better than this baseline accuracy. \n",
    "\n",
    "Hint: To answer the second question, it will be useful to write one additional line of code in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer:\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4 (4 points)**: As we discussed in class, it's important to split data into two non-overlapping parts called the \"training set\" and the \"test set\". We use the data in the training set to \"train the classifier\" where the classifier \"learns\" the relationship between the features and the labels. We can then use the separate test set, in order to assess how accurately the classifier can make predictions on new data. \n",
    "\n",
    "To split the data into training and tests, we can use the scikit-learning function  `X_train, X_test, y_train, y_test = train_test_split(X,  y, random_state = 0)`, where the arguments to the function are:\n",
    "- `X`: A DataFrame (or matrix) of data features\n",
    "- `y`: A Series (or ndarray) of labels\n",
    "- `random_state`: A number that determines which \"random points\" are in the training and test set (this makes our results reproducible by generating the same random splits). \n",
    "\n",
    "The output to this function are:\n",
    "- `X_train`: A DataFrame with the training features\n",
    "- `X_test`:  A DataFrame with the test features\n",
    "- `y_train`: A Series with the training labels\n",
    "- `y_test`:  A Series with the test labels\n",
    "\n",
    "Please go ahead and run the `train_test_split()` function to split the data into a training and test set (using `random_state = 0` argument). Once you have done this, print out the number of points that are in the training set and the number of points that are in the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5 (4 points)**: Now that we have training and test data sets, we are ready to train a classifier. Here let's use the k-nearest neighbors classifier (KNN) that we discussed in class. \n",
    "\n",
    "To create a KNN classifier, we can use the `KNeighborsClassifier(n_neighbors = )` constructor. Please call this constructor to create a KNN classifier using one neighbor and save the result to the name `knn`. \n",
    "\n",
    "Once you have created this classifier, please train the classifier using the `.fit(X, y)` method where `X` and `y` are the training features and training labels. \n",
    "\n",
    "Note: training the KNN classifier (i.e., calling the `.fit()` method) just involves having the classifier save the training and test data for future use. This data is saved by (what is now a \"trained\") the classifier so that we can now use it to make predictions on new data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6 (4 points)**: Now that our KNN classifier has been trained (i.e., now that the KNN classifier has saved the training data), we can use it to make predictions!  \n",
    "\n",
    "To get predictions, we can use the `.predict()` method, which takes a set of features and returns an ndarray of predicted labels.  Please make predictions on the test data and save the predicted labels to the name `predicted_labels`. Print out the predicted labels to see what the predictions are. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.7 (4 points)**: Let's now see how accurate the predictions are. To do this, compare them to the actual test labels, and print out the \"prediction accuracy\" which is the proportion of data points that the classifier predicted correctly. In the answer section, report if this value is higher than what we might expect if the classifier was making the same optimal guess all the time, and a classifier that was randomly guessing, and why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: (if you want to simulate other classifier behavior, you can use this cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.8 (4 points)**: In question 1.6 we had a trained KNN classifier make predictions, and in step 1.7 we used these predictions to assess the classifier's accuracy. Since these two steps are used so frequently in combination, the scikit-learn package has a single method called `.score()` that does both these steps together. In particular, if you call the `.score()` method on a trained KNN classifier giving it test data features and test labels as the arguments, it will return the classification accuracy. \n",
    "\n",
    "Please use this `.score()` method on our trained 1-nearest neighbor classifier (i.e., the `knn` classifier you trained in question 1.5) to get the classification accuracy, and print out the value below. The value should match the accuracy you reported in question 1.7. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.9 (4 points)**: Let's now explore what happens if we tested our KNN classifier on the same data that we trained it on. To do this, use the `.score()` method on our 1-nearest neighbor classifier, but, instead of passing the test features and test labels into the `.score()` method as arguments, pass the training features and training labels. \n",
    "\n",
    "In the answer section, please write answers to the following questions: \n",
    "\n",
    "1. In 1-5 sentences, explain why the classification accuracy is perfect. \n",
    "\n",
    "2. If we had trained and tested the classifier on the same data, but used a value of k = 3 neighbors, would the classification accuracy still be perfect?  Also, in this scenario, would the classification accuracy be a good proxy for how well the classifier would do on new test data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.10 (4 points)**:  So far we have just used a single training and test split of the data to train and test the KNN classifier. However, as discussed in class, in order to get more robust results we can use k-fold cross-validation where we split the data set into k parts. Once we have split the data into k parts, we then train the classifier on k-1 of these parts and test it on the remaining part. We then repeat this process k times, leaving out a different test set each time and training on the remaining data.\n",
    "\n",
    "To run a cross-validation analysis, we can use the scikit-learn `cross_val_score(classifier, features, labels, cv = num_cv_splits)` function. This function takes the following arguments: \n",
    "\n",
    "1. An untrained classifier\n",
    "2. A set of features for each data point\n",
    "3. The class labels for each data point\n",
    "4. The `cv` argument should be set to the number of cross-validation splits (k) that we would like to use. \n",
    "\n",
    "Please create a new 1-nearest neighbor classifier and save it to the name `knn` (overwriting your previous classifier). Then use the `cross_val_score()` to run 5-fold cross-validation by passing it this new KNN classifier, all your features, all your labels, and setting the `cv = ` argument appropriately. Note, since this function is doing the training and test splits for us, we should pass the full original `features_X`,  `labels_y` data, rather than the `X_train`, `X_test`, `y_train`, `y_test` splits we created ourselves. \n",
    "\n",
    "Save the results from running the `cross_val_score()` to the name `scores` and print out the values in the `scores` ndarray which are the classification accuracies for each cross-validation split (i.e., 5 numbers for each time the classifier was trained and tested). Also, print out the mean classification accuracy over the 5 cross-validation splits to show the overall classification accuracy. \n",
    "\n",
    "\n",
    "Note: the `k` in k-fold cross-validation has no relationship to the `k` in our KNN classifier. It is just that the letter `k` is used frequently to indicate a particular integer value (so unfortunately using the symbol `k` as notation is overloaded here). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.11 (4 points)**: In all our analyses so far we have also only used a 1-nearest neighbor classifier. Let's now assess how the classification accuracy changes if we change the number of neighbors that are used to classify each test data point (i.e., if we change the value of `k` in our k-nearest neighbors classifier). \n",
    "\n",
    "To do this, please use a for loop to iterate through different numbers of neighbors in your KNN classifier. Specifically, please do the following:\n",
    "\n",
    "1. Create an empty list called `neighborhood_size_scores` that will store the results from using the KNN classifier with 1 to 9 neighbors (i.e. k values from 1 to 9).\n",
    "2. Use a for loop that loops over the values of `k` from 1 to 9 where in each iteration of the loop you: \n",
    "\n",
    "a. Build a new KNN classifier that uses `k` neighbors.      \n",
    "b. Use the `cross_val_score()` function to run 5-fold cross-validation on the current KNN classifier (i.e., for the current value of `k` neighbors). This can be done using very similar code to question 1.10.    \n",
    "c. Append the average cross-validation accuracy from the current KNN classifier to the `neighborhood_size_scores` list.\n",
    "\n",
    "Once you have done this, print out the `neighborhood_size_scores` list to show the classification accuracies obtained from different values of `k` neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.12 (4 points)**: Let's now visualize how the results changed with different values of k neighbors (i.e., visualize the results you got on the previous problem). To do this, plot the results using matplotlib. Please be sure that the values on your axes are appropriate and that you label your axes (as always!). \n",
    "\n",
    "Then, in the answer section, report what the best number of neighbors to use appears to be (i.e., what is the best `k` in KNN to use for this data). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experimenting with additional classifiers\n",
    "\n",
    "As a final set of exercises, let's briefly explore a couple of additional classifiers. In order to understand how these classifiers work, you will need to take a machine learning class. However, even without understanding how they work, we can still use them to see if they give better classification performance compared to a KNN classifier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (4 points)**: Let's start by comparing our KNN results to results from a Support Vector Machine (SVM). To do this, use the following steps:\n",
    "\n",
    "1. Create a support vector machine called `svm` using the `LinearSVC()` constructor  \n",
    "2. Use the `cross_val_score()` function to run a 5-fold cross-validation and save the scores to the name `svm_scores` (hint: this is very similar to what you did in part 1.10). \n",
    "\n",
    "Print out the mean cross-validation SVM scores to show you have the right answer. In the answer section below, report whether the SVM gave better classification results compared to the KNN classifier you got in question 1.11.   \n",
    "\n",
    "Note: There is some code below that suppresses a few warning messages. Please ignore this code (and ignore any warnings that you might get about \"Liblinear failed to converge\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress ConvergenceWarning - please ignore this code. You can use it if you want to see the warnings.\n",
    "#import warnings\n",
    "#from sklearn.exceptions import ConvergenceWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (4 points)**: Finally, let's try a random forest classifier (for an idea of how this classifier works, [see this diagram](https://i.pinimg.com/originals/02/21/18/02211856c8ed1c5d01cce6855545a9f8.jpg)). To do this, please use the following steps which are very similar to what we used for our Support Vector Machine:\n",
    "\n",
    "1. Create a random forest classifier called `random_forest` using the `RandomForestClassifier()` constructor \n",
    "\n",
    "2. Use the `cross_val_score()` function to run a 5-fold cross-validation and save the scores to the name `forest_scores` (hint: this is very similar to what you did in part 1.10). \n",
    "\n",
    "Print out the mean cross-validation Random Forest scores to show you have the right answer. In the answer section below, report whether the Random Forest gave better classification results compared to the SVM and KNN classifier you got in questions 2.1 and 1.11.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Profit!\n",
    "\n",
    "Now that you know how to use classifiers, you can use them to compete in classification competitions on websites such as https://www.kaggle.com/.  This site has a range of competitions some of which pay out prize money (currently it looks like the prizes range up to $1,000,000).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reflection (3 points)\n",
    "\n",
    "Please reflect on how the homework went by going to Canvas, going to the Quizzes link, and clicking on reflection on homework 9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submission\n",
    "\n",
    "Please submit your assignment as a .pdf on Gradescope. You can access Gradescope through Canvas on the left-hand side of the class home page. The problems in each homework assignment are numbered. **NOTE:** When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. **Failure to mark pages correctly will result in points being deducted from your homework score.**\n",
    "\n",
    "If you are running Jupyter Notebooks through an Anaconda installation on your own computer, you can produce the .pdf by completing the following steps:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "If you are running the assignment on Google Colab, you can use the following instructions: \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook and select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "2. From the print window, select the option to save as a .pdf\n",
    "3. Be sure to look over the pdf file to make sure all your code and written work is saved in a clear way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question (no credit)\n",
    "\n",
    "To avoid a long assignment, this is optional. If you're interested, you can try this because it can be helpful for building classifiers that perform better. It is worth 0 points toward your grade, but if you are interested in building more accuracy machine learning models then you might find the material valuable (particularly if you are interested in competing in a kaggle competition). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bonus section: Transforming features\n",
    "\n",
    "If you look at the features we have been using in our analyses so far, you will notice that they are on very different scales. This is quite problematic for a KNN classifier since the classifier is finding the distance between each data point, so features that have large values will dominate this distance. \n",
    "\n",
    "Let's explore the scales that different features have by looking at some descriptive statistics. In particular, let's go back to the manually created `X_train`, `X_test`, `y_train`, `y_test` to examine the scale that different features are measured on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b.1 (0 points)**:  To start this set of analyses, call the `.describe()` method on training data you create in question 1.4 to see summary statistics on the training data and print out the results. Based on these summary statistics, in the answer section please report which features will essentially be ignored by our KNN classifier since they have value ranges that are very small relative to other features in our data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b.2 (0 points)**: In order to have all features contribute to the KNN classification, it is useful to standardize the features by transforming them to the same scale. In particular, we can use a \"z-score\" transformation, where each feature is transformed to have a mean of 0 and a standard deviation of 1. The way this is done is that the mean and standard deviation are learned on the training features, and then these means and standard deviations are used to apply a z-score transformation to both the training and test features. \n",
    "\n",
    "To do a z-score transformation of our features, we can use the `StandardScaler()` object. In particular, please do the following: \n",
    "\n",
    "1. Create a new `StandardScaler()` object using `scalar = StandardScaler()` \n",
    "2. Have the `scalar` object learn the means and standard deviations of our training data by calling the `scalar.fit(X)` function on the training data you created in question 1.4. \n",
    "\n",
    "Once we have fit our `scalar` object, we can apply it to transform both the training and test features so that all features are on a similar scale. To do this, call the `.transform(X)` method on the training data created in part 1.4 and save the results to the name `X_train_transformed`. Also, apply the `.transform(X)` method on the test data created in part 1.4 and save the results to the name `X_test_transformed`. \n",
    "\n",
    "Finally, print the type of the `X_train_transformed` data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b.3 (0 points)**: In order to see that the z-score transformation worked, please convert the `X_train_transformed` data to a pandas DataFrame using the `pd.DataFrame()` method, and save the results to the name `X_train_transformed_df`. Make sure that the the column names of the  `X_train_transformed_df` match the original column names by using the `columns` argument to the `pd.DataFrame()` function. \n",
    "\n",
    "Once you have created the `X_train_transformed_df` use the `.describe()` method to print the descriptive statistics on the data. In the answer section, report whether the means and standard deviations of the transformed features are what you expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b.4 (0 points)**: Now let's see if z-score transforming the data helped improve our classification performance using a 1-nearest neighbor classifier. To do this, please do the following steps: \n",
    "\n",
    "1. Create a new 1-nearest neighbor classifier named `knn`.\n",
    "2. Fit this classifier on the transformed training data and the training labels\n",
    "3. Assess the classification accuracy on the test data using the `.score()` method.\n",
    "\n",
    "In the answer section, report whether transforming the features seemed to help by comparing the results you get to the classification results you got without transforming the features (i.e., the results you got on question 1.7 and 1.8). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b.5 (0 points)**: In order to transform our features inside a cross-validation loop, we can set up a pipeline. This pipeline will do the following:\n",
    "\n",
    "1. It will split the data into a training and test set\n",
    "2. It will fit the transformation of the features on the training set (i.e., learn the means and standard deviations on the training set). \n",
    "3. It will apply a z-score transformation of the training and test set based on the features learned in step 2\n",
    "4. It will train the classifier on the transformed data\n",
    "5. It will measure the classification accuracy on the test data\n",
    "6. It will repeat this process k times, where k here refers to how many cross-validation splits we are using\n",
    "\n",
    "In order to do this in scikit-learn we can use a `Pipeline` object which sets up the stages of transformation and classification, along with a `KFold` object which will run the cross-validation. \n",
    "\n",
    "The code below does this all for you, and returns an ndarray which has the classification accuracy from each cross-validation split in the name `scores`. Please just print out the mean of these scores to show the average score you get over cross-validation splits. \n",
    "\n",
    "In the answer section, now report whether transforming the features seemed to help the average cross-validated results by comparing the results you got here to the average cross-validation results you got when you did not transform the features (i.e., the average results you got on question 1.10). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b.6 (0 points)**: Let's test how the results vary as a function of using different numbers of neighbors in our KNN classifier on the **transformed data**. To do this, start by creating an empty list called  `neighborhood_size_scores2` to store the results. Then create a loop that loops over different KNN classifiers using 1 to 9 neighbors and inside the loop do the following: \n",
    "\n",
    "\n",
    "1. Build a new KNN classifier that uses `k` neighbors based on the current iteration of the loop\n",
    "2. Build a new pipeline with our scalar transformer and classifier. This can be done using code that is very similar to question 2.5\n",
    "4. Calculate the cross-validation scores using the `cross_val_score()` function. Again, this can use code that is very similar to question 2.5\n",
    "4. Append the average cross-validation accuracy from the current KNN classifier to the `neighborhood_size_scores2` list.\n",
    "\n",
    "To show your code worked, please print out the results scored in `neighborhood_size_scores2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b.7 (0 points)**: Finally, let's visualize all our results from the different numbers of neighbors on the untransformed and transformed data as line plots using matplotlib. Be sure to label your axes and have a legend indicating which results are from the normalized and unnormalized features. In the answer section, report whether overall normalizing the data helped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus section: Experimenting with additional classifiers\n",
    "\n",
    "Finally, let's briefly explore a couple of additional classifiers using a pipeline as we did above, but this time let's apply feature normalization in our pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question bb.1 (0 points)**: Let's start by comparing our KNN results to results from a Support Vector Machine (SVM). To do this, use the following steps:\n",
    "\n",
    "1. Create a support vector machine called `svm` using the `LinearSVC()` constructor \n",
    "2. Create a pipeline, as in question 2.5, where the a scalar z-score tranformation is used, followed by using the SVM we created \n",
    "3. Use the `cross_val_score()` function to get the cross-validation scores and save the scores to the name `svm_scores`\n",
    "\n",
    "Print out the mean cross-validation SVM scores to show you have the right answer. In the answer section below, report whether the SVM gave better classification results compared to the KNN classifier you got in question b.5.   \n",
    "\n",
    "\n",
    "Note: There is some code below the suppresses a few warning messages. Please ignore this code (and ignore any warnings that you might get about \"Liblinear failed to converge\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress ConvergenceWarning - please ignore this code \n",
    "#import warnings\n",
    "#from sklearn.exceptions import ConvergenceWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question bb.2 (0 points)**: Let's also try a random forest classifier including a z-score normalization step in our pipeline. To do this, please use the following steps (which are very similar to what we did above):\n",
    "\n",
    "1. Create a random forest classifier called `random_forest` using the `RandomForestClassifier()` constructor \n",
    "2. Create a pipeline, as in question b.5, where the scalar z-score transformation is used, followed by using the Random Forest Classifier we created \n",
    "3. Use the `cross_val_score()` function to get the cross-validation scores and save the scores to the name `forest_scores`\n",
    "\n",
    "Print out the mean cross-validation Random Forest scores to show you have the right answer. In the answer section below, report whether the Random Forest gave better classification results compared to the SVM and KNN classifier you got in questions b.5 and bb.1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Answer**:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ydata_2025spring_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
